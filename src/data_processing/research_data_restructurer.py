"""
Research Data Restructurer

Fixes data structure issues to enable proper statistical validation:
1. Creates user-personality mappings
2. Adds cultural classification columns
3. Extracts bridge songs in testable format
4. Creates user-level features for clustering validation
5. Enables proper statistical testing of research claims

This bridges the gap between Phase 3 discoveries and statistical validation.
"""

import pandas as pd
import numpy as np
import json
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import warnings
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass

warnings.filterwarnings('ignore')

@dataclass
class RestructureResult:
    """Result from data restructuring process"""
    users_added: int
    personalities_mapped: int
    cultural_scores_added: bool
    bridge_songs_extracted: int
    validation_ready: bool
    issues: List[str]

class ResearchDataRestructurer:
    """
    Restructures research data to enable statistical validation.
    
    Transforms Phase 3 discoveries into testable data structures
    that can be validated with proper statistical rigor.
    """
    
    def __init__(self, random_seed: int = 42):
        self.random_seed = random_seed
        np.random.seed(random_seed)
        self.logger = self._setup_logging()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def restructure_for_validation(self, 
                                 streaming_data: pd.DataFrame,
                                 phase3_results: Dict,
                                 output_path: str = "data/processed/") -> RestructureResult:
        """
        Complete data restructuring for statistical validation.
        
        Args:
            streaming_data: Main streaming dataset
            phase3_results: Phase 3 analysis results
            output_path: Where to save restructured data
            
        Returns:
            Summary of restructuring process
        """
        
        self.logger.info("ğŸ”§ Starting comprehensive data restructuring...")
        issues = []
        
        # Make a copy to avoid modifying original
        enhanced_data = streaming_data.copy()
        
        # Step 1: Create user identifications
        self.logger.info("ğŸ‘¤ Step 1: Creating user identification system...")
        enhanced_data = self._create_user_system(enhanced_data)
        
        # Step 2: Add cultural classification scores
        self.logger.info("ğŸŒ Step 2: Adding cultural classification scores...")
        enhanced_data, cultural_added = self._add_cultural_scores(enhanced_data, phase3_results)
        if not cultural_added:
            issues.append("Cultural scores could not be added reliably")
        
        # Step 3: Create user-personality mappings
        self.logger.info("ğŸ­ Step 3: Creating user-personality mappings...")
        personality_mappings, personalities_mapped = self._create_personality_mappings(
            enhanced_data, phase3_results
        )
        
        # Step 4: Extract and structure bridge songs
        self.logger.info("ğŸŒ‰ Step 4: Extracting bridge songs...")
        bridge_songs_structured, bridge_count = self._extract_bridge_songs(phase3_results)
        
        # Step 5: Create user-level features for clustering
        self.logger.info("ğŸ“Š Step 5: Creating user-level features...")
        user_features = self._create_user_features(enhanced_data, personality_mappings)
        
        # Step 6: Add temporal sequence information
        self.logger.info("â° Step 6: Adding temporal sequence data...")
        enhanced_data = self._add_temporal_sequences(enhanced_data)
        
        # Step 7: Validate data structure for statistical tests
        self.logger.info("âœ… Step 7: Validating restructured data...")
        validation_passed = self._validate_restructured_data(
            enhanced_data, personality_mappings, bridge_songs_structured, user_features
        )
        
        # Save restructured data
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save main enhanced dataset
        enhanced_data.to_parquet(output_dir / "streaming_data_enhanced.parquet")
        
        # Save supporting data structures
        with open(output_dir / "personality_mappings.json", 'w') as f:
            json.dump(personality_mappings, f, indent=2, default=str)
            
        with open(output_dir / "bridge_songs_structured.json", 'w') as f:
            json.dump(bridge_songs_structured, f, indent=2)
            
        user_features.to_parquet(output_dir / "user_features.parquet")
        
        result = RestructureResult(
            users_added=enhanced_data['user_id'].nunique(),
            personalities_mapped=personalities_mapped,
            cultural_scores_added=cultural_added,
            bridge_songs_extracted=bridge_count,
            validation_ready=validation_passed,
            issues=issues
        )
        
        self._print_restructure_summary(result)
        
        return result\n    \n    def _create_user_system(self, data: pd.DataFrame) -> pd.DataFrame:\n        """\n        Create consistent user identification system.\n        \n        Since we don't have user IDs in streaming history, create them based on\n        listening patterns, IP addresses, and temporal clustering.\n        """\n        \n        # Create user sessions based on IP address, platform, and temporal gaps\n        data = data.sort_values('played_at').reset_index(drop=True)\n        \n        # Group by IP address and platform as proxy for users\n        data['user_session'] = (data['ip_addr'].astype(str) + '_' + \n                               data['platform'].fillna('unknown').astype(str))\n        \n        # Detect session breaks (gaps > 4 hours indicate different listening sessions)\n        data['time_diff'] = data.groupby('user_session')['played_at'].diff()\n        data['session_break'] = (data['time_diff'] > pd.Timedelta(hours=4)) | data['time_diff'].isna()\n        data['session_id'] = data.groupby('user_session')['session_break'].cumsum()\n        \n        # Create final user ID combining session and temporal patterns\n        data['user_id'] = (data['user_session'].astype(str) + '_session_' + \n                          data['session_id'].astype(str))\n        \n        # Keep only users with substantial listening history (20+ tracks for statistical power)\n        user_counts = data['user_id'].value_counts()\n        valid_users = user_counts[user_counts >= 20].index\n        data = data[data['user_id'].isin(valid_users)].copy()\n        \n        # Assign sequential user numbers for easier analysis\n        unique_users = data['user_id'].unique()\n        user_mapping = {user: f'user_{i:04d}' for i, user in enumerate(unique_users)}\n        data['user_id'] = data['user_id'].map(user_mapping)\n        \n        self.logger.info(f"âœ… Created {len(unique_users)} users with 20+ tracks each")\n        \n        return data\n    \n    def _add_cultural_scores(self, data: pd.DataFrame, phase3_results: Dict) -> Tuple[pd.DataFrame, bool]:\n        """\n        Add cultural classification scores to streaming data.\n        \n        Uses Phase 3 results and artist/track analysis to assign cultural scores.\n        """\n        \n        try:\n            # Initialize cultural scores\n            data['vietnamese_score'] = 0.0\n            data['western_score'] = 0.0\n            data['chinese_score'] = 0.0\n            data['cultural_classification'] = 'unknown'\n            \n            # Get artist patterns from Phase 3 if available\n            vietnamese_artists = set()\n            western_artists = set()\n            chinese_artists = set()\n            \n            # Extract cultural patterns from personalities\n            if 'musical_personalities_20250828_224450' in phase3_results:\n                personalities = phase3_results['musical_personalities_20250828_224450'].get('personalities', {})\n                \n                for pid, pdata in personalities.items():\n                    top_artists = pdata.get('top_artists', [])\n                    if isinstance(top_artists, dict):\n                        artists = list(top_artists.keys())\n                    else:\n                        artists = top_artists\n                    \n                    # Classify artists by cultural signals\n                    for artist in artists:\n                        if self._is_vietnamese_artist(artist):\n                            vietnamese_artists.add(artist)\n                        elif self._is_chinese_artist(artist):\n                            chinese_artists.add(artist)\n                        else:\n                            western_artists.add(artist)\n            \n            # Also use track names for cultural classification\n            for idx, row in data.iterrows():\n                artist = row.get('artist_name', '')\n                track = row.get('track_name', '')\n                \n                # Calculate cultural scores based on artist and track characteristics\n                viet_score = self._calculate_vietnamese_score(artist, track, vietnamese_artists)\n                western_score = self._calculate_western_score(artist, track, western_artists)\n                chinese_score = self._calculate_chinese_score(artist, track, chinese_artists)\n                \n                # Normalize scores to sum to 1\n                total = viet_score + western_score + chinese_score\n                if total > 0:\n                    viet_score /= total\n                    western_score /= total\n                    chinese_score /= total\n                else:\n                    # Default scores if no cultural signals\n                    viet_score = 0.33\n                    western_score = 0.33\n                    chinese_score = 0.33\n                \n                data.at[idx, 'vietnamese_score'] = viet_score\n                data.at[idx, 'western_score'] = western_score\n                data.at[idx, 'chinese_score'] = chinese_score\n                \n                # Assign dominant cultural classification\n                if viet_score > 0.4:\n                    data.at[idx, 'cultural_classification'] = 'vietnamese'\n                elif western_score > 0.4:\n                    data.at[idx, 'cultural_classification'] = 'western'\n                elif chinese_score > 0.4:\n                    data.at[idx, 'cultural_classification'] = 'chinese'\n                else:\n                    data.at[idx, 'cultural_classification'] = 'mixed'\n            \n            # Validate cultural distribution\n            cultural_dist = data['cultural_classification'].value_counts()\n            self.logger.info(f"âœ… Cultural distribution: {cultural_dist.to_dict()}")\n            \n            return data, True\n            \n        except Exception as e:\n            self.logger.error(f"âŒ Failed to add cultural scores: {e}")\n            return data, False\n    \n    def _is_vietnamese_artist(self, artist: str) -> bool:\n        """Detect Vietnamese artists based on name patterns"""\n        vietnamese_indicators = [\n            'buitruonglinh', 'den vau', 'hoang thuy linh', 'son tung', 'my tam',\n            'duc phuc', 'erik', 'jack', 'k-icm', 'chi pu', 'min', 'amee',\n            'justatee', 'rhymastic', 'binz', 'karik', 'suboi', 'phan manh quynh',\n            'vu cat tuong', 'toc tien', 'bao thy'\n        ]\n        \n        artist_lower = artist.lower()\n        return any(indicator in artist_lower for indicator in vietnamese_indicators)\n    \n    def _is_chinese_artist(self, artist: str) -> bool:\n        """Detect Chinese artists based on name patterns and characters"""\n        chinese_indicators = [\n            'é‚“ç´«æ£‹', 'å‘¨æ°ä¼¦', 'æ—ä¿Šæ°', 'ç‹åŠ›å®', 'å¼ å­¦å‹', 'é™ˆå¥•è¿…', 'æè£æµ©',\n            'è–›ä¹‹è°¦', 'æ¯›ä¸æ˜“', 'åæ™¨å®‡', 'å¼ æ°', 'æ—å®¥å˜‰', 'çŸ³ç™½å…¶', 'è‹æ˜Ÿå©•'\n        ]\n        \n        # Check for Chinese characters\n        chinese_chars = any('\\u4e00' <= char <= '\\u9fff' for char in artist)\n        chinese_names = any(indicator in artist for indicator in chinese_indicators)\n        \n        return chinese_chars or chinese_names\n    \n    def _calculate_vietnamese_score(self, artist: str, track: str, viet_artists: set) -> float:\n        """Calculate Vietnamese cultural score for a track"""\n        score = 0.0\n        \n        # Artist-based scoring\n        if artist in viet_artists or self._is_vietnamese_artist(artist):\n            score += 0.7\n        \n        # Track name-based scoring (Vietnamese words/patterns)\n        track_lower = track.lower()\n        vietnamese_words = ['em', 'anh', 'yeu', 'con', 'khong', 'noi', 'tim', 've', 'cho', 'den']\n        viet_word_count = sum(1 for word in vietnamese_words if word in track_lower)\n        score += min(0.3, viet_word_count * 0.1)\n        \n        return min(1.0, score)\n    \n    def _calculate_western_score(self, artist: str, track: str, western_artists: set) -> float:\n        """Calculate Western cultural score for a track"""  \n        score = 0.0\n        \n        # Artist-based scoring\n        if artist in western_artists or self._is_western_artist(artist):\n            score += 0.7\n        \n        # English language patterns\n        english_indicators = ['the', 'and', 'of', 'to', 'in', 'is', 'you', 'me', 'my', 'love']\n        track_lower = track.lower()\n        english_word_count = sum(1 for word in english_indicators if word in track_lower)\n        score += min(0.3, english_word_count * 0.05)\n        \n        return min(1.0, score)\n    \n    def _is_western_artist(self, artist: str) -> bool:\n        """Detect Western artists based on name patterns"""\n        western_indicators = [\n            'taylor swift', 'ed sheeran', 'bruno mars', 'ariana grande', 'justin bieber',\n            'billie eilish', 'drake', 'the weeknd', 'dua lipa', 'adele', 'sam smith',\n            'john legend', 'alicia keys', 'beyonce', 'rihanna', 'christina grimmie',\n            'austin mahone', 'shawn mendes', 'charlie puth', 'maroon 5'\n        ]\n        \n        artist_lower = artist.lower()\n        return any(indicator in artist_lower for indicator in western_indicators)\n    \n    def _calculate_chinese_score(self, artist: str, track: str, chinese_artists: set) -> float:\n        """Calculate Chinese cultural score for a track"""\n        score = 0.0\n        \n        # Artist-based scoring\n        if artist in chinese_artists or self._is_chinese_artist(artist):\n            score += 0.7\n        \n        # Chinese character detection in track name\n        chinese_chars = sum(1 for char in track if '\\u4e00' <= char <= '\\u9fff')\n        score += min(0.3, chinese_chars * 0.1)\n        \n        return min(1.0, score)\n    \n    def _create_personality_mappings(self, data: pd.DataFrame, phase3_results: Dict) -> Tuple[Dict, int]:\n        """\n        Create user-personality mappings based on listening patterns.\n        \n        Maps users to musical personalities discovered in Phase 3.\n        """\n        \n        personality_mappings = {\n            'user_personalities': {},\n            'personality_definitions': {},\n            'user_features': {}\n        }\n        \n        try:\n            # Get personality definitions from Phase 3\n            if 'musical_personalities_20250828_224450' in phase3_results:\n                personalities = phase3_results['musical_personalities_20250828_224450'].get('personalities', {})\n                personality_mappings['personality_definitions'] = personalities\n                \n                # Calculate user features for personality assignment\n                for user_id in data['user_id'].unique():\n                    user_data = data[data['user_id'] == user_id]\n                    \n                    if len(user_data) < 10:  # Skip users with too little data\n                        continue\n                    \n                    # Calculate user's listening patterns\n                    user_features = self._calculate_user_personality_features(user_data)\n                    personality_mappings['user_features'][user_id] = user_features\n                    \n                    # Assign personality based on similarity to discovered patterns\n                    assigned_personality = self._assign_user_personality(user_features, personalities)\n                    personality_mappings['user_personalities'][user_id] = assigned_personality\n                \n                mapped_users = len(personality_mappings['user_personalities'])\n                self.logger.info(f"âœ… Mapped {mapped_users} users to personalities")\n                \n                # Show personality distribution\n                personality_dist = {}\n                for personality in personality_mappings['user_personalities'].values():\n                    personality_dist[personality] = personality_dist.get(personality, 0) + 1\n                \n                self.logger.info(f"ğŸ“Š Personality distribution: {personality_dist}")\n                \n                return personality_mappings, mapped_users\n            \n            else:\n                self.logger.warning("âš ï¸ No personality data found in Phase 3 results")\n                return personality_mappings, 0\n                \n        except Exception as e:\n            self.logger.error(f"âŒ Failed to create personality mappings: {e}")\n            return personality_mappings, 0\n    \n    def _calculate_user_personality_features(self, user_data: pd.DataFrame) -> Dict[str, float]:\n        """Calculate features that characterize a user's musical personality"""\n        \n        features = {}\n        \n        # Cultural preference ratios\n        features['vietnamese_ratio'] = user_data['vietnamese_score'].mean()\n        features['western_ratio'] = user_data['western_score'].mean()\n        features['chinese_ratio'] = user_data['chinese_score'].mean()\n        \n        # Top artists diversity\n        top_artists = user_data['artist_name'].value_counts().head(10)\n        features['artist_concentration'] = (top_artists.iloc[0] / len(user_data)) if len(top_artists) > 0 else 0\n        features['unique_artists'] = user_data['artist_name'].nunique()\n        \n        # Temporal patterns\n        features['hour_variance'] = user_data['hour'].var() if 'hour' in user_data.columns else 12\n        features['listening_consistency'] = len(user_data) / (user_data['date'].nunique() if 'date' in user_data.columns else 1)\n        \n        # Cultural exploration\n        cultural_types = user_data['cultural_classification'].nunique()\n        features['cultural_diversity'] = cultural_types / 4.0  # Normalized to 0-1\n        \n        return features\n    \n    def _assign_user_personality(self, user_features: Dict, personalities: Dict) -> str:\n        """Assign user to personality based on feature similarity"""\n        \n        if not personalities:\n            return 'unknown'\n        \n        # Calculate similarity to each personality\n        similarities = {}\n        \n        for pid, pdata in personalities.items():\n            similarity = 0.0\n            \n            # Cultural alignment\n            if 'cultural_profile' in pdata:\n                cultural_profile = pdata['cultural_profile']\n                \n                for culture, weight in cultural_profile.items():\n                    if f'{culture.lower()}_ratio' in user_features:\n                        user_ratio = user_features[f'{culture.lower()}_ratio']\n                        similarity += weight * user_ratio\n            \n            # Artist alignment (simplified)\n            if user_features.get('artist_concentration', 0) > 0.3:  # User likes few artists\n                similarity += pdata.get('strength', 0) * 0.5\n            \n            similarities[pid] = similarity\n        \n        # Assign to personality with highest similarity\n        if similarities:\n            return max(similarities.keys(), key=lambda k: similarities[k])\n        else:\n            return list(personalities.keys())[0]  # Default to first personality\n    \n    def _extract_bridge_songs(self, phase3_results: Dict) -> Tuple[Dict, int]:\n        """Extract and structure bridge songs for statistical testing"""\n        \n        bridge_structure = {\n            'bridge_songs': [],\n            'bridge_lookup': {},\n            'effectiveness_data': []\n        }\n        \n        try:\n            if 'cultural_bridges_20250828_224450' in phase3_results:\n                bridge_data = phase3_results['cultural_bridges_20250828_224450']\n                \n                if 'bridge_candidates' in bridge_data:\n                    bridge_candidates = bridge_data['bridge_candidates']\n                    \n                    for bridge in bridge_candidates:\n                        if isinstance(bridge, dict):\n                            # Structure bridge song data for testing\n                            bridge_info = {\n                                'track_name': bridge.get('track_name', ''),\n                                'artist_name': bridge.get('artist_name', ''),\n                                'bridge_score': bridge.get('bridge_score', 0),\n                                'cultural_from': bridge.get('cultural_from', 'unknown'),\n                                'cultural_to': bridge.get('cultural_to', 'unknown'),\n                                'transition_strength': bridge.get('transition_strength', 0),\n                                'track_id': f"{bridge.get('track_name', '')}_{bridge.get('artist_name', '')}".replace(' ', '_').lower()\n                            }\n                            \n                            bridge_structure['bridge_songs'].append(bridge_info)\n                            \n                            # Create lookup for fast access\n                            track_key = bridge_info['track_id']\n                            bridge_structure['bridge_lookup'][track_key] = bridge_info\n                \n                bridge_count = len(bridge_structure['bridge_songs'])\n                self.logger.info(f"âœ… Extracted {bridge_count} bridge songs")\n                \n                return bridge_structure, bridge_count\n            \n            else:\n                self.logger.warning("âš ï¸ No bridge song data found in Phase 3 results")\n                return bridge_structure, 0\n                \n        except Exception as e:\n            self.logger.error(f"âŒ Failed to extract bridge songs: {e}")\n            return bridge_structure, 0\n    \n    def _create_user_features(self, data: pd.DataFrame, personality_mappings: Dict) -> pd.DataFrame:\n        """Create user-level features for clustering validation"""\n        \n        user_features_list = []\n        \n        for user_id in data['user_id'].unique():\n            user_data = data[data['user_id'] == user_id]\n            \n            if len(user_data) < 10:\n                continue\n            \n            # Basic features\n            features = {\n                'user_id': user_id,\n                'total_tracks': len(user_data),\n                'unique_artists': user_data['artist_name'].nunique(),\n                'unique_tracks': user_data['track_name'].nunique(),\n            }\n            \n            # Cultural features\n            features['vietnamese_preference'] = user_data['vietnamese_score'].mean()\n            features['western_preference'] = user_data['western_score'].mean()\n            features['chinese_preference'] = user_data['chinese_score'].mean()\n            features['cultural_diversity'] = user_data['cultural_classification'].nunique() / 4.0\n            \n            # Temporal features\n            if 'hour' in user_data.columns:\n                features['avg_listening_hour'] = user_data['hour'].mean()\n                features['listening_hour_variance'] = user_data['hour'].var()\n            \n            # Personality assignment\n            features['assigned_personality'] = personality_mappings['user_personalities'].get(user_id, 'unknown')\n            \n            # Listening intensity\n            date_range = (user_data['played_at'].max() - user_data['played_at'].min()).days\n            features['listening_intensity'] = len(user_data) / max(1, date_range)\n            \n            user_features_list.append(features)\n        \n        user_features_df = pd.DataFrame(user_features_list)\n        \n        self.logger.info(f"âœ… Created features for {len(user_features_df)} users")\n        \n        return user_features_df\n    \n    def _add_temporal_sequences(self, data: pd.DataFrame) -> pd.DataFrame:\n        """Add temporal sequence information for bridge song analysis"""\n        \n        # Sort by user and time\n        data = data.sort_values(['user_id', 'played_at']).reset_index(drop=True)\n        \n        # Add sequence position within user\n        data['sequence_position'] = data.groupby('user_id').cumcount()\n        \n        # Add previous and next track cultural classifications\n        data['prev_cultural_classification'] = data.groupby('user_id')['cultural_classification'].shift(1)\n        data['next_cultural_classification'] = data.groupby('user_id')['cultural_classification'].shift(-1)\n        \n        # Identify cultural transitions\n        data['is_cultural_transition'] = (\n            (data['cultural_classification'] != data['prev_cultural_classification']) &\n            (data['prev_cultural_classification'].notna())\n        )\n        \n        self.logger.info("âœ… Added temporal sequence information")\n        \n        return data\n    \n    def _validate_restructured_data(self, \n                                  enhanced_data: pd.DataFrame,\n                                  personality_mappings: Dict,\n                                  bridge_songs: Dict,\n                                  user_features: pd.DataFrame) -> bool:\n        """Validate that restructured data is ready for statistical testing"""\n        \n        validation_passed = True\n        issues = []\n        \n        # Check basic data structure\n        required_columns = ['user_id', 'vietnamese_score', 'western_score', 'cultural_classification']\n        missing_columns = [col for col in required_columns if col not in enhanced_data.columns]\n        \n        if missing_columns:\n            issues.append(f"Missing required columns: {missing_columns}")\n            validation_passed = False\n        \n        # Check user-personality mappings\n        if len(personality_mappings.get('user_personalities', {})) < 30:\n            issues.append("Too few users mapped to personalities for clustering validation")\n            validation_passed = False\n        \n        # Check cultural distribution\n        cultural_dist = enhanced_data['cultural_classification'].value_counts()\n        min_cultural_group = cultural_dist.min() if len(cultural_dist) > 0 else 0\n        \n        if min_cultural_group < 50:\n            issues.append(f"Smallest cultural group has only {min_cultural_group} samples")\n            validation_passed = False\n        \n        # Check bridge songs\n        if len(bridge_songs.get('bridge_songs', [])) < 10:\n            issues.append("Too few bridge songs for effectiveness testing")\n            validation_passed = False\n        \n        # Check user features\n        if len(user_features) < 20:\n            issues.append("Too few users with features for clustering validation")\n            validation_passed = False\n        \n        # Report validation results\n        if validation_passed:\n            self.logger.info("âœ… Data validation PASSED - ready for statistical testing")\n        else:\n            self.logger.warning("âš ï¸ Data validation FAILED:")\n            for issue in issues:\n                self.logger.warning(f"   - {issue}")\n        \n        return validation_passed\n    \n    def _print_restructure_summary(self, result: RestructureResult):\n        """Print comprehensive summary of restructuring process"""\n        \n        print("\\n" + "="*60)\n        print("ğŸ”§ DATA RESTRUCTURING COMPLETE")\n        print("="*60)\n        \n        print(f"ğŸ‘¤ Users created: {result.users_added}")\n        print(f"ğŸ­ Users mapped to personalities: {result.personalities_mapped}")\n        print(f"ğŸŒ Cultural scores added: {'âœ…' if result.cultural_scores_added else 'âŒ'}")\n        print(f"ğŸŒ‰ Bridge songs extracted: {result.bridge_songs_extracted}")\n        print(f"âœ… Validation ready: {'YES' if result.validation_ready else 'NO'}")\n        \n        if result.issues:\n            print(f"\\nâš ï¸ Issues detected:")\n            for issue in result.issues:\n                print(f"   - {issue}")\n        \n        if result.validation_ready:\n            print("\\nğŸ¯ Data is now ready for comprehensive statistical validation!")\n            print("   Run: python validate_research_claims.py")\n        else:\n            print("\\nğŸ” Additional data processing needed for full validation")\n            print("   Some tests may have limited statistical power")\n\ndef restructure_research_data(streaming_file: str = None, \n                            phase3_dir: str = None,\n                            output_path: str = "data/processed/") -> RestructureResult:\n    """Convenience function to restructure research data"""\n    \n    # Load data\n    if streaming_file:\n        streaming_data = pd.read_parquet(streaming_file)\n    else:\n        from utils import load_research_data\n        streaming_data, _, phase3_results = load_research_data()\n        \n        if streaming_data is None:\n            raise ValueError("Could not load streaming data")\n    \n    # Load Phase 3 results if not provided\n    if not phase3_results and phase3_dir:\n        phase3_results = {}\n        phase3_path = Path(phase3_dir)\n        \n        for json_file in phase3_path.glob('*.json'):\n            try:\n                with open(json_file, 'r') as f:\n                    data = json.load(f)\n                    phase3_results[json_file.stem] = data\n            except Exception as e:\n                print(f"Warning: Could not load {json_file.name}: {e}")\n    \n    # Restructure\n    restructurer = ResearchDataRestructurer()\n    return restructurer.restructure_for_validation(streaming_data, phase3_results, output_path)\n\nif __name__ == "__main__":\n    print("ğŸ”§ Research Data Restructuring Tool")\n    print("="*50)\n    \n    result = restructure_research_data(\n        streaming_file="data/processed/streaming_data_processed.parquet",\n        phase3_dir="results/phase3/",\n        output_path="data/processed/"\n    )\n    \n    print(f"\\nâœ… Restructuring complete! Result: {result}")